{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FDpQ2u_78BzU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauramanor/CV/blob/master/breastcancer_KNNandDTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**[Can AI Diagnose Breast Cancer?](https://www.sciencebuddies.org/science-fair-projects/project_ideas/ArtificialIntelligence_p010/artificial-intelligence/KNN-breast-cancer)**\n",
        "\n",
        "---\n",
        "https://www.sciencebuddies.org/science-fair-projects/project_ideas/ArtificialIntelligence_p010/artificial-intelligence/KNN-breast-cancer\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This notebook was developed by Science Buddies [www.sciencebuddies.org](https://www.sciencebuddies.org/) as part of a science project to allow students to explore and learn about artificial intelligence. For personal use, this notebook can be downloaded and modified with attribution. For all other uses, please see our [Terms and Conditions of Fair Use](https://www.sciencebuddies.org/about/terms-and-conditions-of-fair-use).  \n",
        "\n",
        "Ms. Manor has heavil edited this notebook for Gold Band CS 2024!\n",
        "\n",
        "**Troubleshooting tips**\n",
        "*   Read the written instructions at Science Buddies and the text and comments on this page carefully.\n"
      ],
      "metadata": {
        "id": "t7Vt7EyejXzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Importing Libraries**\n",
        "We will start this science project by importing some necessary libraries. These libraries contain functions that we will be using to create and display our maze. The comments tell you what each libary is for."
      ],
      "metadata": {
        "id": "uR-gPkhejsG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The pandas library allows us to work with data like spreadsheets.\n",
        "# It helps us organize, clean, and analyze data easily\n",
        "import pandas as pd\n",
        "\n",
        "# Set various display options for pandas to show all columns, rows, and remove width limitations.\n",
        "pd.set_option(\"display.max_columns\", None)    # Display all columns without limit.\n",
        "pd.set_option(\"display.max_rows\", None)       # Display all rows without limit.\n",
        "pd.set_option(\"display.width\", None)          # Remove width restrictions for displaying data.\n",
        "pd.set_option(\"display.max_colwidth\", None)   # Display columns with unlimited width.\n",
        "\n",
        "\n",
        "# For doing math and working with numbers, this library is like a super calculator.\n",
        "# It's great for handling big sets of numbers and doing fancy math operations.\n",
        "import numpy as np\n",
        "\n",
        "# This function helps us convert categorical data into numerical values.\n",
        "# It assigns a unique integer to each category, making it suitable for many algorithms that require numerical input\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# We often want to test how well our \"smart\" programs work.\n",
        "# This library helps us split our data into parts: one for teaching the program and another for testing it.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# A machine learning algorithms used for classification tasks.\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# A metric used to evaluate the performance of classification models by measuring the proportion of correctly\n",
        "# predicted instances in the total instances\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# When we want to draw graphs and charts to show our data visually, we use this.\n",
        "# It helps us see patterns and trends in the data.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Imagine you have a lot of information about people, like their height, weight, age, etc.\n",
        "# Sometimes there's too much information, and we use this to simplify it while keeping the important stuff\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "print(\"You have imported all the libraries\")"
      ],
      "metadata": {
        "id": "MTonvhPAjt-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv(\"https://www.sciencebuddies.org/ai/colab/breastcancer.csv?t=AQXm10t69CcSJQgDXixMI7XQnL9jwNxi10NnK6MvgKMxYg\")\n",
        "\n",
        "# We can see what the data frame looks like by using the head function\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Dc589zTajz8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Preprocessing the data**"
      ],
      "metadata": {
        "id": "xE0q4GxGk3la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Drop unnecesary columns\n",
        "We will first drop features that we think will be uninformative for modeling. In our case, we will be dropping the ID column because it simply serves as a unique identifier for each row and doesn't provide any meaningful information for the analysis or modeling."
      ],
      "metadata": {
        "id": "XEhAfEv9k5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the ID column\n",
        "data.drop('id', axis=1, inplace=True)\n",
        "\n",
        "# Let's check if that column is now gone\n",
        "data.head()\n",
        "\n",
        "# Remember, if you get a KeyError, it is likely becuase you already dropped the column. Try reloading the data and trying again!"
      ],
      "metadata": {
        "id": "LQjq2maek5I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Encode and Separate the Target Variable\n",
        "\n",
        "We will be trying to diagnose cancer, so let's take a closer look at the diagnosis column."
      ],
      "metadata": {
        "id": "cew0qJuZlLih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['diagnosis'].unique()"
      ],
      "metadata": {
        "id": "erIXywnFlOls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are only two options for diagnosis. For binary classification problems, we can use label encoding."
      ],
      "metadata": {
        "id": "a2g-giGqlTO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the target data using label encoding\n",
        "data['diagnosis'] = label_encoder.fit_transform(data['diagnosis'])\n",
        "\n",
        "# Let's check what the dataframe looks like now!\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kQZX5f7-lU52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets separate the diagnosis into the its own dataframe (y) and drop it from the original dataframe."
      ],
      "metadata": {
        "id": "Z5ivXg52lkJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the 'diagnosis' column (target variable) to the variable 'y'. This will be the value we aim to predict\n",
        "y = data['diagnosis']\n",
        "\n",
        "# Drop the column from the dataframe'axis=1' indicates we are dropping a column\n",
        "data = data.drop('diagnosis', axis=1)"
      ],
      "metadata": {
        "id": "Vkvh8MdzllXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Normalization\n",
        "\n",
        "It is usually important to normalize or scale the features to ensure that all features contribute equally to any distance calculations. Let's make a new data frame, `normalized` so that we can compare the normalized results to the standard results.\n",
        "\n",
        "Since all of our remaining columns are numerical, we can just normalize all columns!"
      ],
      "metadata": {
        "id": "XI4Um9zKk8qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy of the original data set\n",
        "normalized = data.copy()\n",
        "\n",
        "# Apply min-max scaling to the entire datafreme\n",
        "normalized = (normalized - normalized.min()) / (normalized.max() - normalized.min())"
      ],
      "metadata": {
        "id": "uiFTlKypk_oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see what our normalization did! Do you remember which function we used to look at the data frame?\n",
        "normalized.head()"
      ],
      "metadata": {
        "id": "hRcrXuSdlKUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Train-Test Split\n",
        "\n",
        "Train-Test Split: Now that we have finished preprocessing our data, it is now time to split our data into training and testing datasets. The training dataset is used to train the KNN model, and the testing dataset is used to evaluate its performance on unseen data."
      ],
      "metadata": {
        "id": "bjrHlIV3pTCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
        "X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(normalized, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "jn1j7-cw7Kuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TODO #1:**\n",
        "* The shapes should be exactly the same for both versions, though the data may look slightly different. Print out the shapes of the normalized splits below to confirm"
      ],
      "metadata": {
        "id": "egEqwPGyu-fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shapes of the resulting datasets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "# TODO: print the shapes of the normalized splits, similar to how we split the non-normalized above."
      ],
      "metadata": {
        "id": "J5_wmDmSpmUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. The KNN Model**\n",
        "\n",
        "### 3.1 KNN without Normalization\n",
        "\n",
        "I have completed the code for the non-normalized data below. You will be asked to finish the code for and compare the two different types of pre-processing.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FDpQ2u_78BzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the value of 'k', which represents the number of neighbors to consider for each prediction\n",
        "k = 5\n",
        "\n",
        "# Create an instance of the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "# Fit the KNN classifier to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the KNN model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "id": "Od3QB_d18FZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 KNN with Normalization\n",
        "\n",
        "#### **TODO #2**:\n",
        "* Compelte the code for the normalized data below"
      ],
      "metadata": {
        "id": "5RgKKkjLqN--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the value of 'k', which represents the number of neighbors to consider for each prediction\n",
        "k = 5\n",
        "\n",
        "# Create an instance of the KNN classifier\n",
        "\n",
        "# Fit the KNN classifier to the training data\n",
        "\n",
        "# Make predictions on the testing data\n",
        "\n",
        "# Evaluate the performance of the KNN model\n"
      ],
      "metadata": {
        "id": "UzWKJCNKp4a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TODO #3**:\n",
        "* Answer within this text box: Does normalization matter? Which preprocessing method worked better? Be detailed - if there is one metric that is driving your decision, make sure to be explicit about what the metric MEANS in this context.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**DOUBLE CLICK HERE TO WRITE YOUR ANSWER**\n",
        "\n"
      ],
      "metadata": {
        "id": "lNkTRIIzqWzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.3 Comparing different neighbor sizes using a loop\n",
        "\n",
        "#### **TODO #4**\n",
        "* update the training data based on which model preformed better above"
      ],
      "metadata": {
        "id": "GmqIKwil8ZXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a range of values for the number of neighbors\n",
        "neighbors = np.arange(1, 21)\n",
        "\n",
        "# Initialize an empty list to store accuracy scores\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "\n",
        "# Iterate over different values of neighbors\n",
        "for n in neighbors:\n",
        "    # Create an instance of the KNN classifier with the current number of neighbors\n",
        "    knn = KNeighborsClassifier(n_neighbors=n)\n",
        "\n",
        "    # Fit the KNN classifier to the training data\n",
        "    # TODO: use the correct trainig data based on your obervations above\n",
        "    knn.fit(___,____)\n",
        "\n",
        "    # Make predictions on the testing data\n",
        "    y_pred = knn.predict(_____)\n",
        "\n",
        "    # Compute the accuracy score and append it to the list\n",
        "    # note that y_test should not have to change because they should be exactly the same! (since we used a random seed for the splitting)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    precision_scores.append(precision)\n",
        "\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    recall_scores.append(recall)"
      ],
      "metadata": {
        "id": "Z6EzfFQX8dGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy scores\n",
        "plt.plot(neighbors, accuracy_scores, label='accuracy')\n",
        "plt.plot(neighbors, precision_scores, label='precision')\n",
        "plt.plot(neighbors, recall_scores, label='recall')\n",
        "plt.xlabel('Number of Neighbors')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Performance Metrics for Different Numbers of Neighbors')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YAQe4Bdt8ggY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TODO #5**:\n",
        "* Answer within this text box: Which K would you chose for your release? Why? if there is one metric that is driving your decision, make sure to be explicit about what the metric MEANS in this context.\n",
        "\n",
        "\n",
        "\n",
        "**DOUBLE CLICK HERE TO WRITE YOUR ANSWER**\n",
        "\n"
      ],
      "metadata": {
        "id": "L4ZLLqLLvSt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. The Decision Tree Model**\n"
      ],
      "metadata": {
        "id": "owDsev1Bky6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the Decision Tree classifier\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Fit the KNN classifier to the training data\n",
        "dtc.fit(X_train,y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = dtc.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the KNN model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "id": "HDrZKOu2k61A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = list(y.unique())\n",
        "feature_names = list(data.columns)"
      ],
      "metadata": {
        "id": "gun185n5w0Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_text\n",
        "r = export_text(dtc, feature_names=feature_names, show_weights=True)\n",
        "print(r)"
      ],
      "metadata": {
        "id": "PYAKA3MPxoFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the tree created by the model. Make some observations and at least one argument based on the results. Example, which features seem to be the mores important when it comes to detecting cancer? Why? Do you think all of the branches are needed? Why? Why not?\n",
        "\n",
        "Remember that you can find descriptions of each feature here: https://www.sciencebuddies.org/science-fair-projects/project_ideas/ArtificialIntelligence_p010/artificial-intelligence/KNN-breast-cancer\n",
        "\n",
        "#### **TODO #6**:\n",
        "* Answer within this text box: Make at least one argument based off of the results of the decision tree\n",
        "\n",
        "\n",
        "\n",
        "**DOUBLE CLICK HERE TO WRITE YOUR ANSWER**\n",
        "\n"
      ],
      "metadata": {
        "id": "4ZAVTFsM0LFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges\n",
        "Once you have completed all 6 TODOs in this notebook, you are invited to try one (or all!) of these challenges. Please leave the notebook above as-is, and add any new code/notes below.\n",
        "\n",
        "\n",
        "### Mild:\n",
        "Take a look at the [documentation for the decision tree classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). Do some experiments with the parameters (e.g. criterion, max_depth, etc). Then, write a reflection on which parameters seem to made a difference on the results and why.\n",
        "\n",
        "\n",
        "### Medium:\n",
        "Use the Decision Tree Classifier to analyze the titanic data! See your old colab notebook for the link to the data and make sure to preprocess the data properly. (But please put your code here so I don't have to go and search for it.)\n",
        "\n",
        "\n",
        "### Spicy:\n",
        "1. Go to the sklearn website and find a classifier that we have not yet worked with! Implement it! Try to do a bit of resesarch and figure out how it works.\n",
        "\n",
        "\n",
        "2. Go to the sklearn website and find a new dataset to run either the KNN or Decisition tree on! https://scikit-learn.org/stable/user_guide.html make sure to do any preprocessing etc!"
      ],
      "metadata": {
        "id": "721JPKia8iBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to add as many code blocks  and text blocks as you need for your experiments!"
      ],
      "metadata": {
        "id": "aGCtQp3G1uaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}